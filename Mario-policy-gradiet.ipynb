{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a6c66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "from gym.wrappers import FrameStack\n",
    "from torchvision import transforms\n",
    "import gym_super_mario_bros\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from torch.distributions import Categorical\n",
    "from gym.spaces import Box\n",
    "import matplotlib.pyplot as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c83f8fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(low=0, high=255, shape=self.observation_space.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transform = transforms.Grayscale()\n",
    "        return transform(torch.tensor(np.transpose(observation, (2, 0, 1)).copy(), dtype=torch.float))\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        self.shape = (shape, shape)\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transformations = transforms.Compose([transforms.Resize(self.shape), transforms.Normalize(0, 255)])\n",
    "        return transformations(observation).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "011276c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\wrappers\\record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at D:\\COMP_Topics_i_AI\\ProjectB\\pg\\video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "env = FrameStack(ResizeObservation(GrayScaleObservation(SkipFrame(env, skip=4)), shape=84), num_stack=4)\n",
    "env = gym.wrappers.RecordVideo(env, 'D:/COMP_Topics_i_AI/ProjectB/pg/video', episode_trigger = lambda x: True)\n",
    "env.seed(42)\n",
    "env.action_space.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f46ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioSolver:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, env.action_space.n),\n",
    "            nn.Softmax(dim=-1)\n",
    "        ).cuda()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate, eps=1e-4)\n",
    "        self.reset()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def reset(self):\n",
    "        self.episode_actions = torch.tensor([], requires_grad=True).cuda()\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def save_checkpoint(self, directory, episode):\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        filename = os.path.join(directory, 'checkpoint_{}.pth'.format(episode))\n",
    "        torch.save(self.model.state_dict(), f=filename)\n",
    "        print('Checkpoint saved to \\'{}\\''.format(filename))\n",
    "\n",
    "    def load_checkpoint(self, directory, filename):\n",
    "        self.model.load_state_dict(torch.load(os.path.join(directory, filename)))\n",
    "        print('Resuming training from checkpoint \\'{}\\'.'.format(filename))\n",
    "        return int(filename[11:-4])\n",
    "\n",
    "    def backward(self):\n",
    "        future_reward = 0\n",
    "        rewards = []\n",
    "        for r in self.episode_rewards[::-1]:\n",
    "            future_reward = r + gamma * future_reward\n",
    "            rewards.append(future_reward)\n",
    "        rewards = torch.tensor(rewards[::-1], dtype=torch.float32).cuda()\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "        loss = torch.sum(torch.mul(self.episode_actions, rewards).mul(-1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "208f3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "gamma = 0.95\n",
    "load_filename = 'checkpoint_2000.pth'\n",
    "save_directory = 'D:/COMP_Topics_i_AI/ProjectB/pg'\n",
    "batch_rewards = []\n",
    "episode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f06b32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from checkpoint 'checkpoint_2000.pth'.\n",
      "loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 201, average reward: 626.9\n",
      "Batch: 202, average reward: 709.2\n",
      "Batch: 203, average reward: 709.7\n",
      "Batch: 204, average reward: 737.2\n",
      "Batch: 205, average reward: 701.0\n",
      "Batch: 206, average reward: 784.0\n",
      "Batch: 207, average reward: 633.5\n",
      "Batch: 208, average reward: 667.9\n",
      "Batch: 209, average reward: 663.3\n",
      "Batch: 210, average reward: 594.5\n",
      "Batch: 211, average reward: 698.2\n",
      "Batch: 212, average reward: 741.5\n",
      "Batch: 213, average reward: 674.8\n",
      "Batch: 214, average reward: 482.9\n",
      "Batch: 215, average reward: 765.4\n",
      "Batch: 216, average reward: 488.7\n",
      "Batch: 217, average reward: 785.0\n",
      "Batch: 218, average reward: 748.4\n",
      "Batch: 219, average reward: 704.1\n",
      "Batch: 220, average reward: 669.9\n",
      "Batch: 221, average reward: 864.0\n",
      "Batch: 222, average reward: 580.9\n",
      "Batch: 223, average reward: 848.4\n",
      "Batch: 224, average reward: 997.9\n",
      "Batch: 225, average reward: 715.2\n",
      "Batch: 226, average reward: 641.5\n",
      "Batch: 227, average reward: 764.8\n",
      "Batch: 228, average reward: 688.9\n",
      "Batch: 229, average reward: 762.9\n",
      "Batch: 230, average reward: 607.4\n",
      "Batch: 231, average reward: 811.2\n",
      "Batch: 232, average reward: 788.8\n",
      "Batch: 233, average reward: 660.0\n",
      "Batch: 234, average reward: 661.7\n",
      "Batch: 235, average reward: 558.4\n",
      "Batch: 236, average reward: 667.4\n",
      "Batch: 237, average reward: 568.4\n",
      "Batch: 238, average reward: 566.8\n",
      "Batch: 239, average reward: 678.9\n",
      "Batch: 240, average reward: 703.4\n",
      "Batch: 241, average reward: 848.8\n",
      "Batch: 242, average reward: 541.1\n",
      "Batch: 243, average reward: 788.3\n",
      "Batch: 244, average reward: 753.3\n",
      "Batch: 245, average reward: 526.5\n",
      "Batch: 246, average reward: 756.0\n",
      "Batch: 247, average reward: 700.5\n",
      "Batch: 248, average reward: 505.2\n",
      "Batch: 249, average reward: 642.3\n",
      "Batch: 250, average reward: 896.5\n",
      "Batch: 251, average reward: 692.1\n",
      "Batch: 252, average reward: 678.6\n",
      "Batch: 253, average reward: 696.3\n",
      "Batch: 254, average reward: 523.4\n",
      "Batch: 255, average reward: 580.2\n",
      "Batch: 256, average reward: 821.9\n",
      "Batch: 257, average reward: 759.8\n",
      "Batch: 258, average reward: 1019.0\n",
      "Batch: 259, average reward: 874.7\n",
      "Batch: 260, average reward: 780.8\n",
      "Batch: 261, average reward: 605.5\n",
      "Batch: 262, average reward: 608.7\n",
      "Batch: 263, average reward: 682.2\n",
      "Batch: 264, average reward: 550.4\n",
      "Batch: 265, average reward: 655.7\n",
      "Batch: 266, average reward: 848.6\n",
      "Batch: 267, average reward: 922.5\n",
      "Batch: 268, average reward: 524.6\n",
      "Batch: 269, average reward: 724.6\n",
      "Batch: 270, average reward: 755.8\n",
      "Batch: 271, average reward: 881.1\n",
      "Batch: 272, average reward: 748.8\n",
      "Batch: 273, average reward: 507.8\n",
      "Batch: 274, average reward: 836.5\n",
      "Batch: 275, average reward: 642.2\n",
      "Batch: 276, average reward: 701.8\n",
      "Batch: 277, average reward: 704.1\n",
      "Batch: 278, average reward: 641.9\n",
      "Batch: 279, average reward: 634.8\n",
      "Batch: 280, average reward: 686.6\n",
      "Batch: 281, average reward: 717.5\n",
      "Batch: 282, average reward: 548.5\n",
      "Batch: 283, average reward: 488.4\n",
      "Batch: 284, average reward: 743.5\n",
      "Batch: 285, average reward: 529.8\n",
      "Batch: 286, average reward: 789.8\n",
      "Batch: 287, average reward: 671.4\n",
      "Batch: 288, average reward: 596.8\n",
      "Batch: 289, average reward: 817.0\n",
      "Batch: 290, average reward: 658.5\n",
      "Batch: 291, average reward: 598.5\n",
      "Batch: 292, average reward: 782.9\n",
      "Batch: 293, average reward: 785.7\n",
      "Batch: 294, average reward: 668.7\n",
      "Batch: 295, average reward: 671.6\n",
      "Batch: 296, average reward: 691.4\n",
      "Batch: 297, average reward: 663.9\n",
      "Batch: 298, average reward: 587.0\n",
      "Batch: 299, average reward: 568.9\n",
      "Batch: 300, average reward: 505.1\n",
      "Checkpoint saved to 'D:/COMP_Topics_i_AI/ProjectB/pg\\checkpoint_3000.pth'\n",
      "Batch: 301, average reward: 541.6\n",
      "Batch: 302, average reward: 708.7\n",
      "Batch: 303, average reward: 567.1\n",
      "Batch: 304, average reward: 776.9\n",
      "Batch: 305, average reward: 454.1\n",
      "Batch: 306, average reward: 595.0\n",
      "Batch: 307, average reward: 535.0\n",
      "Batch: 308, average reward: 685.4\n",
      "Batch: 309, average reward: 792.5\n",
      "Batch: 310, average reward: 816.1\n",
      "Batch: 311, average reward: 695.3\n",
      "Batch: 312, average reward: 771.1\n",
      "Batch: 313, average reward: 877.9\n",
      "Batch: 314, average reward: 685.7\n",
      "Batch: 315, average reward: 532.6\n",
      "Batch: 316, average reward: 688.6\n",
      "Batch: 317, average reward: 676.2\n",
      "Batch: 318, average reward: 747.0\n",
      "Batch: 319, average reward: 515.1\n",
      "Batch: 320, average reward: 705.2\n",
      "Batch: 321, average reward: 802.9\n",
      "Batch: 322, average reward: 592.3\n",
      "Batch: 323, average reward: 665.4\n",
      "Batch: 324, average reward: 716.4\n",
      "Batch: 325, average reward: 819.3\n",
      "Batch: 326, average reward: 627.3\n",
      "Batch: 327, average reward: 622.9\n",
      "Batch: 328, average reward: 570.9\n",
      "Batch: 329, average reward: 872.2\n",
      "Batch: 330, average reward: 793.5\n",
      "Batch: 331, average reward: 618.4\n",
      "Batch: 332, average reward: 650.2\n",
      "Batch: 333, average reward: 799.1\n",
      "Batch: 334, average reward: 753.5\n",
      "Batch: 335, average reward: 918.8\n",
      "Batch: 336, average reward: 723.4\n",
      "Batch: 337, average reward: 587.3\n",
      "Batch: 338, average reward: 767.6\n",
      "Batch: 339, average reward: 622.9\n",
      "Batch: 340, average reward: 640.6\n",
      "Batch: 341, average reward: 854.7\n",
      "Batch: 342, average reward: 615.9\n",
      "Batch: 343, average reward: 662.0\n",
      "Batch: 344, average reward: 824.6\n",
      "Batch: 345, average reward: 633.1\n",
      "Batch: 346, average reward: 818.2\n",
      "Batch: 347, average reward: 703.0\n",
      "Batch: 348, average reward: 721.9\n",
      "Batch: 349, average reward: 744.7\n",
      "Batch: 350, average reward: 884.7\n",
      "Batch: 351, average reward: 699.7\n",
      "Batch: 352, average reward: 607.6\n",
      "Batch: 353, average reward: 424.5\n",
      "Batch: 354, average reward: 599.4\n",
      "Batch: 355, average reward: 443.8\n",
      "Batch: 356, average reward: 652.1\n",
      "Batch: 357, average reward: 694.6\n",
      "Batch: 358, average reward: 625.2\n",
      "Batch: 359, average reward: 479.3\n",
      "Batch: 360, average reward: 680.7\n",
      "Batch: 361, average reward: 699.2\n",
      "Batch: 362, average reward: 443.8\n",
      "Batch: 363, average reward: 439.8\n",
      "Batch: 364, average reward: 684.6\n",
      "Batch: 365, average reward: 698.8\n",
      "Batch: 366, average reward: 665.1\n",
      "Batch: 367, average reward: 737.2\n",
      "Batch: 368, average reward: 831.9\n",
      "Batch: 369, average reward: 732.5\n",
      "Batch: 370, average reward: 635.6\n",
      "Batch: 371, average reward: 619.0\n",
      "Batch: 372, average reward: 739.3\n",
      "Batch: 373, average reward: 664.6\n",
      "Batch: 374, average reward: 804.0\n",
      "Batch: 375, average reward: 743.5\n",
      "Batch: 376, average reward: 982.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8584\\3785004449.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mall_mean_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\wrappers\\record_video.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mobservations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRecordVideo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecording\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_video_recorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobservations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\wrappers\\record_video.py\u001b[0m in \u001b[0;36mstart_video_recorder\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m         )\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecorded_frames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecording\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframes_per_sec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_frames_per_sec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             )\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"encoder_version\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36mversion_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    345\u001b[0m             \u001b[1;34m\"backend\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m             \"version\": str(\n\u001b[1;32m--> 347\u001b[1;33m                 subprocess.check_output(\n\u001b[0m\u001b[0;32m    348\u001b[0m                     \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-version\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTDOUT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m                 )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0m\u001b[0;32m    425\u001b[0m                **kwargs).stdout\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m             \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m   1119\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                 \u001b[0mstdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = MarioSolver(learning_rate=0.00025)\n",
    "if load_filename is not None:\n",
    "    episode = model.load_checkpoint(save_directory, load_filename)\n",
    "    print(\"loaded\")\n",
    "all_episode_rewards = []\n",
    "all_mean_rewards = []\n",
    "while True:\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        observation = torch.tensor(observation.__array__()).cuda().unsqueeze(0)\n",
    "        distribution = Categorical(model.forward(observation))\n",
    "        action = distribution.sample()\n",
    "        observation, reward, done, _ = env.step(action.item())\n",
    "        model.episode_actions = torch.cat([model.episode_actions, distribution.log_prob(action).reshape(1)])\n",
    "        model.episode_rewards.append(reward)\n",
    "        if done:\n",
    "            all_episode_rewards.append(np.sum(model.episode_rewards))\n",
    "            batch_rewards.append(np.sum(model.episode_rewards))\n",
    "            model.backward()\n",
    "            episode += 1\n",
    "            if episode % batch_size == 0:\n",
    "                print('Batch: {}, average reward: {}'.format(episode // batch_size, np.array(batch_rewards).mean()))\n",
    "                batch_rewards = []\n",
    "                all_mean_rewards.append(np.mean(all_episode_rewards[-batch_size:]))\n",
    "                if episode % 500 == 0:\n",
    "                    plt.plot(all_mean_rewards)\n",
    "                    plt.savefig(\"{}/mean_reward_{}.png\".format(save_directory, episode))\n",
    "                    plt.clf()\n",
    "            if episode % 1000 == 0 and save_directory is not None:\n",
    "                model.save_checkpoint(save_directory, episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07158159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
